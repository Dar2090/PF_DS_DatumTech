{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3552a2cb-e045-4416-8000-cd22cdd771d1",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Importamos las librerias necesarias para realizar nuestro ETL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "af9c6956-c65a-4fdf-a334-f9dfa284d985",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "from pyspark.sql.functions import col, sum, lower, array_contains, expr, lit, concat_ws, when, monotonically_increasing_id\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e19e84bb-b97d-4545-a656-b65e52f5fa0e",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creacion de tabla `processed_files_business` para mantener el registro de los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3ce9ed63-47a6-424d-9a8f-bec1cc80527a",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "spark.sql(\"CREATE TABLE IF NOT EXISTS processed_files_business_yelp (file_name STRING) USING DELTA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b232a5bd-90aa-475a-a6e4-803c37024d11",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Obtenemos la lista de archivos en ADLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b90cbc61-42ad-439a-a2f9-9ecd3513ae8d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Crear una instancia de SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Especificar el nombre del contenedor y directorio\n",
        "container_name = \"datumtech\"\n",
        "directory_path = \"/Yelp/business\"\n",
        "\n",
        "# Obtener la lista de carpetas\n",
        "adls_files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\n",
        "    .listStatus(spark._jvm.org.apache.hadoop.fs.Path(f\"abfss://{container_name}.blob.core.windows.net/{directory_path}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7590116d-d7c4-4030-b2db-c5f037de4e71",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creamos un variable `new_files` que contiene los archivos que no estan en ADLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4952e931-aeb0-4500-b39b-894655a86af8",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "processed_files = spark.sql(\"SELECT file_name FROM processed_files_business_yelp\").toPandas()[\"file_name\"].tolist()\n",
        "\n",
        "new_files = [file.getPath().getName() for file in adls_files if file.getPath().getName() not in processed_files]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "947295fe-45bc-4221-9f69-ca0668e5081a",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos ver que archivos ya estan procesados en la tabla `processed_files_business`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3573ff47-7e58-4d7b-a838-91a35f82a5b0",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM processed_files_business_yelp\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "999d8908-3428-4a24-98ab-2de9dee9c966",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos ver que archivos no estan procesados aun."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "319a3530-59ed-4d9a-9fd4-0a40c10cdadb",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "for file in new_files:\r\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "09bbff77-bdf5-494b-aa27-3f133d3650e4",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creamos la funcion `etl` que realiza todo el proceso y devuelve el dataframe en formato parquet a la tabla silver corrrespondiente a los datos ya procesados. Tambien definimos algunas variables necesarias para el filtrado y para desanidar alguna columna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5a6d020a-1027-4801-a15d-49c140b4542f",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Variable con categorias a filtrar\n",
        "category = [\n",
        "    'burger', 'burgers', 'hamburger', 'hamburgers' 'hot dog', 'steakhouse', 'lunch', 'motel', 'patisserie', 'pizza', 'deli', 'diner', 'dinner', 'icecream', 'ice cream', 'hotel', 'hotels', 'seafood','cookie', 'crab house', 'cupcake', 'chocolate', 'churreria', 'cocktail', 'cocktails', 'coffee', 'coffees' 'tea', 'restaurant', 'restaurats', 'chesse', 'charcuterie', 'cafe', 'cafes', 'BBQ', 'bagle', 'bakery' 'bakerys', 'bar', 'bars', 'bar & grill', 'barbacue', 'beer' 'bistro', 'pasteleria', 'pastelerias', 'breakfast', 'brunch', 'buffet', 'burrito', 'cafeteria', 'cafeterias', 'cake', 'cakes', 'food']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "37d42872-57c0-4153-9476-7bad8036db26",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "column_names = [\n",
        "    \"AcceptsInsurance\", \"AgesAllowed\", \"Alcohol\", \"Ambience\", \"BYOB\", \"BYOBCorkage\", \"BestNights\", \"BikeParking\", \"BusinessAcceptsBitcoin\", \"BusinessAcceptsCreditCards\", \"BusinessParking\", \"ByAppointmentOnly\", \"Caters\", \"CoatCheck\", \"Corkage\", \"DietaryRestrictions\", \"DogsAllowed\",\n",
        "    \"DriveThru\", \"GoodForDancing\", \"GoodForKids\", \"GoodForMeal\", \"HairSpecializesIn\", \"HappyHour\", \"HasTV\", \"Music\", \"NoiseLevel\", \"Open24Hours\",\n",
        "    \"OutdoorSeating\", \"RestaurantsAttire\", \"RestaurantsCounterService\", \"RestaurantsDelivery\", \"RestaurantsGoodForGroups\", \"RestaurantsPriceRange2\" \"RestaurantsReservations\", \"RestaurantsTableService\", \"RestaurantsTakeOut\", \"Smoking\", \"WheelchairAccessible\", \"WiFi\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "60781467-0b5a-4eac-a7e3-d77f53395a09",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "\n",
        "def etl(file):\n",
        "    \n",
        "    # Definimos las rutas:\n",
        "    path_raw=f\"abfss://datumtech@datumlake.dfs.core.windows.net/Yelp/business/{file}.parquet\"\n",
        "    path_bronze = f\"abfss://datumtech@datumlake.dfs.core.windows.net/bronze/Yelpbronze/business-bronze/{file}-bronze\"\n",
        "    path_silver = f\"abfss://datumtech@datumlake.dfs.core.windows.net/silver/Yelpsilver/business-silver/{file}-silver\"\n",
        "\n",
        "    # Cargamos el archivo desde ADLS. Nos quedamos solo con las columnas consideradas para el proyecto:\n",
        "    df_raw = spark.read.format(\"parquet\").load(path_raw).select('business_id', 'name', 'address', 'state', 'latitude', 'longitude', 'stars', 'review_count','attributes', 'categories')\n",
        "     \n",
        "    # Guardamos el DataFrame df_raw en la tabla bronze correspondiente a los datos en crudo o poco procesados en Azure Data Lake con un formato parquet ideal para manejar altos volumenes de datos.\n",
        "    df_raw.write.format(\"parquet\").save(path_bronze)\n",
        "        \n",
        "    # Cargamos el archivo desde la tabla bronze en Azure Data Lake en un DataFrame.\n",
        "    df_business = spark.read.format(\"parquet\").load(path_bronze)\n",
        "\n",
        "    # Borramos duplicados.\n",
        "    df_business = df_business.dropDuplicates()\n",
        "\n",
        "    # Eliminamos los registros donde 'business_id', 'categories' y 'state' son nulos o vacios. Representan menos del 10% de los datos totales. Ademas 'business_id' sera una PK mas adelante.\n",
        "    df_business = df_business.na.drop(subset=['business_id', 'categories', 'state'])\n",
        "\n",
        "    # Rellenamos valores vacíos o nulos en las columnas 'name', 'address', 'latitude', 'longitude', 'stars', 'review_count'. A pesar de no tener nulos en algunas, dejamos planteado el codigo para usar el notebook en jobs posteriores.\n",
        "    \n",
        "    df_business = df_business.fillna('Unknown', subset=['name', 'address'])\n",
        "    df_business = df_business.fillna(0, subset=['latitude', 'longitude', 'stars', 'review_count'])\n",
        "\n",
        "    # Definimos un filtro para la columna 'categories' asi solo nos quedamos con las categorias necesarias para el proyecto.\n",
        "    filtro = expr(\"lower(categories)\").rlike(r\"\\b(\" + \"|\".join(category) + r\")\\b\")\n",
        "    df_business = df_business.filter(filtro)\n",
        "\n",
        "    # Desanidamos la columna 'attributes', esta informacion nos servira mas adelante en la devolucion de recomandacion de nuestro modelo de ML.\n",
        "    df_attributes = df_business.select(\"attributes\")\n",
        "    df_desanidado = df_attributes.selectExpr(\"attributes.*\")\n",
        "    column_names = df_desanidado.columns\n",
        "\n",
        "    # Filtramos las columnas que tienen valor 'True'\n",
        "    columnas_filtradas = [expr(f\"`{c}` = 'True'\").alias(c) for c in df_desanidado.columns]\n",
        "\n",
        "    # Seleccionamos las columnas filtradas y nos quedamos solo con los atributos true para nuestra columna 'attributes'\n",
        "    df_desanidado = df_desanidado.select(*columnas_filtradas)\n",
        "    concat_expr = concat_ws(\", \", *[when(df_desanidado[col], col) for col in column_names])\n",
        "    df_desanidado = df_desanidado.withColumn(\"new_attributes\", concat_expr)\n",
        "    df_desanidado = df_desanidado.select(\"new_attributes\")\n",
        "\n",
        "    # Agregamos una columna de índice a df_filtrado y a df_desanidado para poder hacer un join\n",
        "    df_business = df_business.withColumn(\"index\", monotonically_increasing_id())\n",
        "    df_desanidado = df_desanidado.withColumn(\"index\", monotonically_increasing_id())\n",
        "\n",
        "    # Realizamos la unión utilizando la columna de índice\n",
        "    df_business = df_business.join(df_desanidado, \"index\").drop(\"index\")\n",
        "\n",
        "    # Eliminamos la columna attributes\n",
        "    df_business = df_business.drop(\"attributes\")\n",
        "\n",
        "    # Renombramos la columna new_attributes a attributes y stars a avg_rating\n",
        "    df_business = df_business.withColumnRenamed(\"new_attributes\", \"attributes\")\n",
        "    df_business = df_business.withColumnRenamed(\"stars\", \"avg_rating\")\n",
        "\n",
        "    # Rellenamos los nulos en la columna \"attributes\" \n",
        "    df_business = df_business.fillna('Unknown', subset=['attributes'])\n",
        "\n",
        "    # Guardamos el DataFrame df_metadata en la tabla silver correspondiente a los datos procesados en Azure Data Lake.\n",
        "    return df_business.write.format(\"parquet\").save(path_silver)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "45ca0f1e-c9ea-41f5-860f-3371d71fad5d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Iteramos sobre cada archivo sin procesar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e97045d1-6c7a-4bcb-86ab-c765f478c4d2",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "for file in new_files:\n",
        "    etl(file.rstrip(\".parquet\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1b5de4c7-c23f-4032-b8e9-5442e40e4b0f",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Agregamos a la tabla `processed_files_business` los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2dfc0dd7-7d0f-4818-9f0d-de05fc7101f6",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "new_files_df = spark.createDataFrame([(file,) for file in new_files], [\"file_name\"])\n",
        "new_files_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"processed_files_business_yelp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ff629741-cace-4a0d-94ed-476f9c6f0054",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos verificar que, efectivamente esten registrados los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "793d5dee-6b48-4949-a242-44b891b18d9b",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM processed_files_business_yelp\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e069d029-6178-43a1-91d8-c496905db21b",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Para comprobar que todo se haya ejecutado de manera correcta, podemos traer cualquier archivo de la tabla silver y hacer algunas verificaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "14a56ad2-25ae-42e7-9095-34bbfed14f99",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "path= f\"abfss://datumtech@datumlake.dfs.core.windows.net/silver/Yelpsilver/business-silver/business-silver\"\n",
        "df = spark.read.format(\"parquet\").load(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "985abc9e-ad21-45bf-bacc-a34d5b2d3af1",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "76a101ca-0bbf-4b98-93bc-aa9068291bc3",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos verificar si hay nulos en algun archivo en la tabla silver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "704b7ffd-91fe-42a6-b1f6-661d412d746d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Funcion para el conteo de nulos del dataframe.\n",
        "def null_counts (df):\n",
        "    counts = df.select(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
        "    return counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f135b019-6319-42b7-af36-cb888a5521b6",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Vemos que columnas poseen nulos y en que cantidad.\n",
        "nulls = null_counts(df)"
      ]
    }
  ],
  "metadata": {
    "description": "En este notebook se automatiza todo el procesamiento de carga, transformacion y exportacion en los archivos de Business_Yelp",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}