{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3552a2cb-e045-4416-8000-cd22cdd771d1",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Importamos las librerias necesarias para realizar nuestro ETL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "af9c6956-c65a-4fdf-a334-f9dfa284d985",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "from pyspark.sql.functions import concat_ws, col, sum, lower, substring, to_date\r\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e19e84bb-b97d-4545-a656-b65e52f5fa0e",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creacion de tabla `processed_files_reviews_yelp` para mantener el registro de los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3ce9ed63-47a6-424d-9a8f-bec1cc80527a",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "spark.sql(\"CREATE TABLE IF NOT EXISTS processed_files_reviews_yelp (file_name STRING) USING DELTA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b232a5bd-90aa-475a-a6e4-803c37024d11",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Obtenemos los archivos sin procesar desde ADLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b90cbc61-42ad-439a-a2f9-9ecd3513ae8d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Crear una instancia de SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Especificar el nombre del contenedor y directorio\n",
        "container_name = \"datumtech\"\n",
        "directory_path = \"/Yelp/reviews\"\n",
        "\n",
        "# Obtener la lista de carpetas\n",
        "adls_files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\n",
        "    .listStatus(spark._jvm.org.apache.hadoop.fs.Path(f\"abfss://{container_name}.blob.core.windows.net/{directory_path}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7590116d-d7c4-4030-b2db-c5f037de4e71",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creamos un variable `new_files` que contiene los archivos que no estan procesados en ADLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4952e931-aeb0-4500-b39b-894655a86af8",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "processed_files = spark.sql(\"SELECT file_name FROM processed_files_reviews_yelp\").toPandas()[\"file_name\"].tolist()\n",
        "\n",
        "new_files = [file.getPath().getName() for file in adls_files if file.getPath().getName() not in processed_files]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "947295fe-45bc-4221-9f69-ca0668e5081a",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos ver que archivos ya estan procesados en la tabla `processed_files_metadata`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3573ff47-7e58-4d7b-a838-91a35f82a5b0",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "#spark.sql(\"SELECT * FROM processed_files_reviews_yelp\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "999d8908-3428-4a24-98ab-2de9dee9c966",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos ver que archivos no estan procesados aun."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "319a3530-59ed-4d9a-9fd4-0a40c10cdadb",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "for file in new_files:\r\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "09bbff77-bdf5-494b-aa27-3f133d3650e4",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creamos la funcion `etl` que realiza todo el proceso y devuelve el dataframe en formato parquet a la tabla silver corrrespondiente a los datos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "60781467-0b5a-4eac-a7e3-d77f53395a09",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "\n",
        "def etl(file):\n",
        "    \n",
        "    # Definimos las rutas:\n",
        "    path_raw=f\"abfss://datumtech@datumlake.dfs.core.windows.net/Yelp/reviews/{file}.json\"\n",
        "    path_bronze = f\"abfss://datumtech@datumlake.dfs.core.windows.net/bronze/Yelpbronze/reviews-bronze/{file}-bronze\"\n",
        "    path_silver = f\"abfss://datumtech@datumlake.dfs.core.windows.net/silver/Yelpsilver/reviews-silver/{file}-silver\"\n",
        "\n",
        "    # Cargamos el archivo desde ADLS. Nos quedamos solo con las columnas consideradas para el proyecto:\n",
        "    df_raw = spark.read.format(\"json\").load(path_raw).select('review_id', 'user_id', 'business_id', 'stars', 'text', 'date')\n",
        "     \n",
        "    # Guardamos el DataFrame df_raw en la tabla bronze correspondiente a los datos en crudo o poco procesados en Azure Data Lake con un formato parquet ideal para manejar altos volumenes de datos.\n",
        "    df_raw.write.format(\"parquet\").save(path_bronze)\n",
        "        \n",
        "    # Cargamos el archivo desde la tabla bronze en Azure Data Lake en un DataFrame.\n",
        "    df_reviews = spark.read.format(\"parquet\").load(path_bronze) \n",
        "\n",
        "    # Eliminar los duplicados\n",
        "    df_reviews = df_reviews.dropDuplicates()\n",
        "\n",
        "    # Eliminamos los registros donde haya nulos o vacios.\n",
        "    df_reviews = df_reviews.na.drop()\n",
        "    \n",
        "    # Extraemos la parte de la fecha que nos interesa utilizando la función substring\n",
        "    df_reviews = df_reviews.withColumn(\"date\", substring(df_reviews.date, 1, 10))\n",
        "\n",
        "    # Aplicamos la conversión a formato de fecha utilizando la función to_date\n",
        "    df_reviews = df_reviews.withColumn(\"date\", to_date(df_reviews.date, \"yyyy-MM-dd\"))\n",
        "\n",
        "    # Guardamos el DataFrame df_metadata en la tabla silver correspondiente a los datos procesados en Azure Data Lake.\n",
        "    return df_reviews.write.format(\"parquet\").save(path_silver)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "45ca0f1e-c9ea-41f5-860f-3371d71fad5d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Iteramos sobre cada archivo sin procesar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e97045d1-6c7a-4bcb-86ab-c765f478c4d2",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "for file in new_files:\n",
        "    etl(file.rstrip(\".json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1b5de4c7-c23f-4032-b8e9-5442e40e4b0f",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Agregamos a la tabla `processed_files_reviews_yelp` los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2dfc0dd7-7d0f-4818-9f0d-de05fc7101f6",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "new_files_df = spark.createDataFrame([(file,) for file in new_files], [\"file_name\"])\n",
        "new_files_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"processed_files_reviews_yelp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ff629741-cace-4a0d-94ed-476f9c6f0054",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos verificar que, efectivamente esten registrados los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "793d5dee-6b48-4949-a242-44b891b18d9b",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "#spark.sql(\"SELECT * FROM processed_files_reviews_yelp\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e069d029-6178-43a1-91d8-c496905db21b",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "#### Para comprobar que todo se haya ejecutado de manera correcta, podemos traer cualquier archivo de la tabla silver y hacer algunas verificaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "14a56ad2-25ae-42e7-9095-34bbfed14f99",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "path = f\"abfss://datumtech@datumlake.dfs.core.windows.net/silver/Yelpsilver/reviews-silver/review-silver\"\n",
        "df = spark.read.format(\"parquet\").load(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "985abc9e-ad21-45bf-bacc-a34d5b2d3af1",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "76a101ca-0bbf-4b98-93bc-aa9068291bc3",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos verificar si hay nulos en algun archivo en la tabla silver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "704b7ffd-91fe-42a6-b1f6-661d412d746d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Funcion para el conteo de nulos del dataframe.\n",
        "def null_counts (df):\n",
        "    counts = df.select([sum(col(c).isNull().cast(\"integer\")).alias(c) for c in df.columns])\n",
        "    return counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f135b019-6319-42b7-af36-cb888a5521b6",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Vemos que columnas poseen nulos y en que cantidad.\n",
        "nulls = null_counts(df)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}