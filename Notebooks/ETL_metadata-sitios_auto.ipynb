{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3552a2cb-e045-4416-8000-cd22cdd771d1",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Importamos las librerias necesarias para realizar nuestro ETL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "af9c6956-c65a-4fdf-a334-f9dfa284d985",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "from pyspark.sql.functions import concat_ws, col, sum, lower, regexp_extract, substring, split, expr, trim, udf\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.types import StringType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e19e84bb-b97d-4545-a656-b65e52f5fa0e",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creacion de tabla `processed_files` para mantener el registro de los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3ce9ed63-47a6-424d-9a8f-bec1cc80527a",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "spark.sql(\"CREATE TABLE IF NOT EXISTS processed_files_metadata_google (file_name STRING) USING DELTA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b232a5bd-90aa-475a-a6e4-803c37024d11",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Obtenemos la lista de archivos contenidos en el storage. En este caso ADLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Crear una instancia de SparkSession\r\n",
        "spark = SparkSession.builder.getOrCreate()\r\n",
        "\r\n",
        "# Especificar el nombre del contenedor y directorio\r\n",
        "container_name = \"datumtech\"\r\n",
        "directory_path = \"/GoogleMaps/metadata-sitios\"\r\n",
        "\r\n",
        "# Obtener la lista de carpetas\r\n",
        "adls_files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\r\n",
        "    .listStatus(spark._jvm.org.apache.hadoop.fs.Path(f\"abfss://{container_name}.blob.core.windows.net/{directory_path}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7590116d-d7c4-4030-b2db-c5f037de4e71",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creamos un variable `new_files` que contiene los archivos que no estan en ADLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4952e931-aeb0-4500-b39b-894655a86af8",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "processed_files = spark.sql(\"SELECT file_name FROM processed_files_metadata_google\").toPandas()[\"file_name\"].tolist()\n",
        "\n",
        "new_files = [file.getPath().getName() for file in adls_files if file.getPath().getName() not in processed_files]\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "947295fe-45bc-4221-9f69-ca0668e5081a",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos ver que archivos ya estan procesados en la tabla `processed_files_metadata_google`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3573ff47-7e58-4d7b-a838-91a35f82a5b0",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM processed_files_metadata_google\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "999d8908-3428-4a24-98ab-2de9dee9c966",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos ver que archivos no estan procesados aun."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "319a3530-59ed-4d9a-9fd4-0a40c10cdadb",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "for file in new_files:\r\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "09bbff77-bdf5-494b-aa27-3f133d3650e4",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creamos la funcion `etl` que realiza todo el proceso y devuelve el dataframe en formato parquet a la tabla silver corrrespondiente a los datos ya procesados y la funcion para cambiar los nombres de los estados por sus abreviaciones. Tambien definimos variables necesarias para hacer filtrados mas adelante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e1fffc50-6c67-447a-96c3-815feb2d690d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Variable con categorias a filtrar\n",
        "category = [\n",
        "    'burger', 'burgers', 'hamburger', 'hamburgers' 'hot dog', 'steakhouse', 'lunch', 'motel', 'patisserie', 'pizza', 'deli', 'diner', 'dinner', 'icecream', 'ice cream', 'hotel', 'hotels', 'seafood','cookie', 'crab house', 'cupcake', 'chocolate', 'churreria', 'cocktail', 'cocktails', 'coffee', 'coffees' 'tea', 'restaurant', 'restaurats', 'chesse', 'charcuterie', 'cafe', 'cafes', 'BBQ', 'bagle', 'bakery' 'bakerys', 'bar', 'bars', 'bar & grill', 'barbacue', 'beer' 'bistro', 'pasteleria', 'pastelerias', 'breakfast', 'brunch', 'buffet', 'burrito', 'cafeteria', 'cafeterias', 'cake', 'cakes', 'food', 'wine', 'wineries',\n",
        "    'lunch'\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Variable con estados y sus abreviaciones\r\n",
        "us_states = [\r\n",
        "    'Alabama', 'AL', 'Alaska', 'AK', 'Arizona', 'AZ', 'Arkansas', 'AR', 'California', 'CA', 'Colorado', 'CO', 'Connecticut', 'CT', 'Delaware', 'DE', 'Florida', 'FL', 'Georgia', 'GA', 'Hawaii', 'HI',\r\n",
        "    'Idaho', 'ID', 'Illinois', 'IL', 'Indiana', 'IN', 'Iowa', 'IA', 'Kansas', 'KS', 'Kentucky', 'KY', 'Louisiana', 'LA', 'Maine', 'ME', 'Maryland', 'MD', 'Massachusetts', 'MA', 'Michigan', 'MI', \r\n",
        "    'Minnesota', 'MN', 'Mississippi', 'MS', 'Missouri', 'MO', 'Montana', 'MT', 'Nebraska', 'NE', 'Nevada', 'NV', 'New Hampshire', 'NH', 'New Jersey', 'NJ', 'New Mexico', 'NM', 'New York', 'NY', \r\n",
        "    'North Carolina', 'NC',  'North Dakota', 'ND', 'Ohio', 'OH', 'Oklahoma', 'OK', 'Oregon', 'OR', 'Pennsylvania', 'PA', 'Rhode Island', 'RI', 'South Carolina', 'SC', 'South Dakota', 'SD', \r\n",
        "    'Tennessee', 'TN', 'Texas', 'TX', 'Utah', 'UT', 'Vermont', 'VT', 'Virginia', 'VA', 'Washington', 'WA', 'West Virginia', 'WV', 'Wisconsin', 'WI', 'Wyoming', 'WY'\r\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Diccionario de correspondencias de estados\r\n",
        "state_mapping = {\r\n",
        "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI',\r\n",
        "    'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA', 'Michigan': 'MI',\r\n",
        "    'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\r\n",
        "    'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD',\r\n",
        "    'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Función para mapear los nombres de los estados a las abreviaciones\r\n",
        "def map_state_name_to_abbr(state_name):\r\n",
        "    if state_name in state_mapping:\r\n",
        "        return state_mapping[state_name]\r\n",
        "    else:\r\n",
        "        return state_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "60781467-0b5a-4eac-a7e3-d77f53395a09",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "\n",
        "def etl(file):\n",
        "    \n",
        "    # Definimos las rutas:\n",
        "    path_raw=f\"abfss://datumtech@datumlake.dfs.core.windows.net/GoogleMaps/metadata-sitios/{file}\"\n",
        "    path_bronze = f\"abfss://datumtech@datumlake.dfs.core.windows.net/bronze/GoogleMapsbronze/metadata-sitios-bronze/{file}-bronze\"\n",
        "    path_silver = f\"abfss://datumtech@datumlake.dfs.core.windows.net/silver/GoogleMapssilver/metadata-sitios-silver/{file}-silver\"\n",
        "\n",
        "    # Cargamos el archivo desde ADLS. Nos quedamos solo con las columnas consideradas para el proyecto:\n",
        "    df_raw = spark.read.format(\"json\").load(path_raw).select('address', 'avg_rating', 'category', 'gmap_id', 'latitude', 'longitude', 'name', 'num_of_reviews', 'url')\n",
        "     \n",
        "    # Guardamos el DataFrame df_raw en la tabla bronze correspondiente a los datos en crudo o poco procesados en Azure Data Lake con un formato parquet ideal para manejar altos volumenes de datos.\n",
        "    df_raw.write.format(\"parquet\").save(path_bronze)\n",
        "        \n",
        "    # Cargamos el archivo desde la tabla bronze en Azure Data Lake en un DataFrame.\n",
        "    df_metadata = spark.read.format(\"parquet\").load(path_bronze)\n",
        "        \n",
        "    # Utilizamos la función concat_ws para obtener los valores del array de la columna 'category' concatenados en el mismo registro\n",
        "    df_metadata = df_metadata.withColumn('values_concatenados', concat_ws(', ', df_metadata.category))  \n",
        "\n",
        "    # Eliminamos la columna \"category\" del DataFrame\n",
        "    df_metadata = df_metadata.drop(\"category\")\n",
        "\n",
        "    # Renombra la columna \"values_concatenados\" a \"nueva_columna\"\n",
        "    df_metadata = df_metadata.withColumnRenamed(\"values_concatenados\", \"category\")    \n",
        "\n",
        "    # Eliminar los duplicados\n",
        "    df_metadata = df_metadata.dropDuplicates()\n",
        "\n",
        "    # Rellenamos valores vacíos o nulos en las columnas 'address', 'name' y 'url'. A pesar de no tener nulos en url, dejamos planteado el codigo para usar el notebook en jobs posteriores\n",
        "    # Eliminamos los registros donde 'avg_rating', 'gmap_id', 'latitude', 'longitude', 'num_of_reviews' y 'category' son nulos o vacios.\n",
        "    df_metadata = df_metadata.fillna('Unknown', subset=['address', 'name'])\n",
        "    df_metadata = df_metadata.na.drop(subset=['avg_rating', 'gmap_id', 'latitude', 'longitude', 'num_of_reviews', 'category']) \n",
        "    \n",
        "    # Filtramos de la columna category las que sean necesarias para el proyecto.\n",
        "    filtro = expr(\"lower(category)\").rlike(r\"\\b(\" + \"|\".join(category) + r\")\\b\")\n",
        "    df_metadata = df_metadata.filter(filtro)\n",
        "\n",
        "    # Creamos la columna 'state' extrayendo el estado de la columna 'address'\n",
        "    state_regex = '|'.join(['\\\\b' + state + '\\\\b' for state in us_states])\n",
        "    df_metadata = df_metadata.withColumn('state', when(col('address').rlike(state_regex), regexp_extract(col('address'), state_regex, 0)).otherwise('Unknown'))\n",
        "\n",
        "    # Registrar la función UDF y aplicamos la transformación a la columna 'state_abbr'\n",
        "    map_state_name_to_abbr_udf = udf(map_state_name_to_abbr, StringType()) \n",
        "    df_metadata = df_metadata.withColumn('state', map_state_name_to_abbr_udf(col('state')))\n",
        "\n",
        "    # Guardamos el DataFrame df_metadata en la tabla silver correspondiente a los datos procesados en Azure Data Lake.\n",
        "    return df_metadata.write.format(\"parquet\").save(path_silver)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "45ca0f1e-c9ea-41f5-860f-3371d71fad5d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Iteramos sobre cada archivo sin procesar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e97045d1-6c7a-4bcb-86ab-c765f478c4d2",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "for file in new_files:\n",
        "    etl(file.rstrip(\"/\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1b5de4c7-c23f-4032-b8e9-5442e40e4b0f",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Agregamos a la tabla `processed_files_metadata_google` los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2dfc0dd7-7d0f-4818-9f0d-de05fc7101f6",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "new_files_df = spark.createDataFrame([(file,) for file in new_files], [\"file_name\"])\n",
        "new_files_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"processed_files_metadata_google\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ff629741-cace-4a0d-94ed-476f9c6f0054",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos verificar que, efectivamente esten registrados los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "793d5dee-6b48-4949-a242-44b891b18d9b",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM processed_files_metadata_google\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e069d029-6178-43a1-91d8-c496905db21b",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Para comprobar que todo se haya ejecutado de manera correcta, podemos traer cualquier archivo de la tabla silver y hacer algunas verificaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "14a56ad2-25ae-42e7-9095-34bbfed14f99",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Funcion para cargar el archivo desde la tabla correspondiente al estado de los datos en Azure Data Lake en un DataFrame.\n",
        "def load_from(file, level):\n",
        "    path= f\"abfss://datumtech@datumlake.dfs.core.windows.net/{level}/GoogleMaps{level}/metadata-sitios-{level}/{file}-{level}\"\n",
        "    df = spark.read.format(\"parquet\").load(path)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "266ce4d7-5dec-4403-93bb-756db86c19dd",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Cambiando ...new_files[<valor de la fila en la tabla>][1]... podras cargar algun archivo de los ya procesados de la tabla silver.\n",
        "df = load_from(new_files[0].rstrip(\"/\"), \"silver\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "76a101ca-0bbf-4b98-93bc-aa9068291bc3",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos verificar si hay nulos en algun archivo en la tabla silver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "704b7ffd-91fe-42a6-b1f6-661d412d746d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Funcion para el conteo de nulos del dataframe.\n",
        "def null_counts (df):\n",
        "    counts = df.select([sum(col(c).isNull().cast(\"integer\")).alias(c) for c in df.columns])\n",
        "    return counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f135b019-6319-42b7-af36-cb888a5521b6",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Vemos que columnas poseen nulos y en que cantidad.\n",
        "nulls = null_counts(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.dtypes"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}