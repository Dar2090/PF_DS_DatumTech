{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3552a2cb-e045-4416-8000-cd22cdd771d1",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Importamos las librerias necesarias para realizar nuestro ETL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "af9c6956-c65a-4fdf-a334-f9dfa284d985",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "from pyspark.sql.functions import concat_ws, col, sum, from_unixtime\r\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e19e84bb-b97d-4545-a656-b65e52f5fa0e",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creacion de tabla `processed_files` para mantener el registro de los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3ce9ed63-47a6-424d-9a8f-bec1cc80527a",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "spark.sql(\"CREATE TABLE IF NOT EXISTS processed_files_reviews_google (file_name STRING) USING DELTA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b232a5bd-90aa-475a-a6e4-803c37024d11",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Obtenemos la lista de archivos contenidos en el storage. En este caso ADLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b90cbc61-42ad-439a-a2f9-9ecd3513ae8d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Crear una instancia de SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Especificar el nombre del contenedor y directorio\n",
        "container_name = \"datumtech\"\n",
        "directory_path = \"/GoogleMaps/reviews-estados/\"\n",
        "\n",
        "# Obtener la lista de carpetas\n",
        "adls_files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\n",
        "    .listStatus(spark._jvm.org.apache.hadoop.fs.Path(f\"abfss://{container_name}.blob.core.windows.net/{directory_path}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7590116d-d7c4-4030-b2db-c5f037de4e71",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creamos un variable `new_files` que contiene los archivos que no estan en ADLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4952e931-aeb0-4500-b39b-894655a86af8",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "processed_files = spark.sql(\"SELECT file_name FROM processed_files_reviews_google\").toPandas()[\"file_name\"].tolist()\n",
        "\n",
        "new_files = [file.getPath().getName() for file in adls_files if file.getPath().getName() not in processed_files]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "947295fe-45bc-4221-9f69-ca0668e5081a",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos ver que archivos ya estan procesados en la tabla `processed_files`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3573ff47-7e58-4d7b-a838-91a35f82a5b0",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM processed_files_reviews_google\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "999d8908-3428-4a24-98ab-2de9dee9c966",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos ver que archivos no estan procesados aun."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "75125273-566b-45ce-b37e-0284a8b48794",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "for file in new_files:\r\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "09bbff77-bdf5-494b-aa27-3f133d3650e4",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creamos la funcion `etl` que realiza todo el proceso y devuelve el dataframe en formato parquet a la tabla silver corrrespondiente a los datos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "60781467-0b5a-4eac-a7e3-d77f53395a09",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "\n",
        "def etl(file):\n",
        "    \n",
        "    # Definimos las rutas:\n",
        "    path_raw=f\"abfss://datumtech@datumlake.dfs.core.windows.net/GoogleMaps/reviews-estados/{file}\"\n",
        "    path_bronze = f\"abfss://datumtech@datumlake.dfs.core.windows.net/bronze/GoogleMapsbronze/reviews-estados-bronze/{file}-bronze\"\n",
        "    path_silver = f\"abfss://datumtech@datumlake.dfs.core.windows.net/silver/GoogleMapssilver/reviews-estados-silver/{file}-silver\"\n",
        "\n",
        "    # Cargamos el archivo desde ADLS. Nos quedamos solo con las columnas consideradas para el proyecto:\n",
        "    df_raw = spark.read.format(\"json\").load(path_raw).select('user_id', 'gmap_id', 'rating', 'text', 'time')\n",
        "     \n",
        "    # Guardamos el DataFrame df_raw en la tabla bronze correspondiente a los datos en crudo o poco procesados en Azure Data Lake con un formato parquet ideal para manejar altos volumenes de datos.\n",
        "    df_raw.write.format(\"parquet\").save(path_bronze)\n",
        "        \n",
        "    # Cargamos el archivo desde la tabla bronze en Azure Data Lake en un DataFrame.\n",
        "    df_review = spark.read.format(\"parquet\").load(path_bronze)\n",
        "\n",
        "    # Eliminar los duplicados\n",
        "    df_review = df_review.dropDuplicates()\n",
        "\n",
        "    # Rellenamos valores vac√≠os o nulos en las columnas 'text', y 'rating'. A pesar de no tener nulos en algunas, dejamos planteado el codigo para usar el notebook en jobs posteriores.\n",
        "    # Eliminamos los registros donde 'gmap_id', 'user_id' y 'time' son nulos o vacios. Representan menos del 10% de los datos totales.\n",
        "    df_review = df_review.fillna('Unknown', subset=['text'])\n",
        "    df_review = df_review.fillna(0, subset=['rating'])\n",
        "    df_review = df_review.na.drop(subset=['user_id', 'gmap_id', 'time'])\n",
        "\n",
        "    # Transformar la columna 'time' en formato de fecha\n",
        "    df_review = df_review.withColumn(\"date\", from_unixtime(df_review.time / 1000).cast(\"date\"))\n",
        "    df_review = df_review.drop('time')\n",
        "\n",
        "    # Guardamos el DataFrame df_review en la tabla silver correspondiente a los datos procesados en Azure Data Lake.\n",
        "    return df_review.write.format(\"parquet\").save(path_silver)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "45ca0f1e-c9ea-41f5-860f-3371d71fad5d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Iteramos sobre cada archivo sin procesar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e97045d1-6c7a-4bcb-86ab-c765f478c4d2",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "for file in new_files:\n",
        "    etl(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1b5de4c7-c23f-4032-b8e9-5442e40e4b0f",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Agregamos a la tabla `processed_files_reviews_google` los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2dfc0dd7-7d0f-4818-9f0d-de05fc7101f6",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "new_files_df = spark.createDataFrame([(file,) for file in new_files], [\"file_name\"])\n",
        "new_files_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"processed_files_reviews_google\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ff629741-cace-4a0d-94ed-476f9c6f0054",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos verificar que, efectivamente esten registrados los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "793d5dee-6b48-4949-a242-44b891b18d9b",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM processed_files_reviews_google\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e069d029-6178-43a1-91d8-c496905db21b",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "#### Para comprobar que todo se haya ejecutado de manera correcta, podemos traer cualquier archivo de la tabla silver y hacer algunas verificaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "14a56ad2-25ae-42e7-9095-34bbfed14f99",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Funcion para cargar el archivo desde la tabla correspondiente al estado de los datos en Azure Data Lake en un DataFrame.\n",
        "def load_from(file, level):\n",
        "    path= f\"abfss://datumtech@datumlake.dfs.core.windows.net/{level}/GoogleMaps{level}/reviews-estados-{level}/{file}-{level}\"\n",
        "    df = spark.read.format(\"parquet\").load(path)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "266ce4d7-5dec-4403-93bb-756db86c19dd",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Cambiando ...new_files[<valor de la fila en la tabla>][1]... podras cargar algun archivo de los ya procesados de la tabla silver.\n",
        "df = load_from(new_files[1], \"silver\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "985abc9e-ad21-45bf-bacc-a34d5b2d3af1",
          "showTitle": false,
          "title": ""
        },
        "collapsed": false
      },
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "76a101ca-0bbf-4b98-93bc-aa9068291bc3",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos verificar si hay nulos en algun archivo en la tabla silver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "704b7ffd-91fe-42a6-b1f6-661d412d746d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Funcion para el conteo de nulos del dataframe.\n",
        "def null_counts (df):\n",
        "    counts = df.select([sum(col(c).isNull().cast(\"integer\")).alias(c) for c in df.columns])\n",
        "    return counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f135b019-6319-42b7-af36-cb888a5521b6",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Vemos que columnas poseen nulos y en que cantidad.\n",
        "nulls = null_counts(df)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}