{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3552a2cb-e045-4416-8000-cd22cdd771d1",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Importamos las librerias necesarias para realizar nuestro proceso de enriquecimiento de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "af9c6956-c65a-4fdf-a334-f9dfa284d985",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "from pyspark.sql.functions import col, substring, concat\r\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Creacion de tabla `processed_files_reviews_google_silver` y `processed_file_metadata_silver`para mantener el registro de los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"CREATE TABLE IF NOT EXISTS processed_files_reviews_google_silver (file_name STRING) USING DELTA\")\r\n",
        "spark.sql(\"CREATE TABLE IF NOT EXISTS processed_files_metadata_google_silver (file_name STRING) USING DELTA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e19e84bb-b97d-4545-a656-b65e52f5fa0e",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Aca podes revisar las tablas que mantienen el registro de los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3ce9ed63-47a6-424d-9a8f-bec1cc80527a",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM processed_files_reviews_google_silver\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM processed_files_metadata_google_silver\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b232a5bd-90aa-475a-a6e4-803c37024d11",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Obtenemos la lista de archivos contenidos en el storage. En este caso los contenidos en la carpeta silver que corresponden a los archivos luego del proceso de ETL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b90cbc61-42ad-439a-a2f9-9ecd3513ae8d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Crear una instancia de SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Especificar el nombre del contenedor y directorio\n",
        "container_name = \"datumtech\"\n",
        "directory_path_reviews = \"/silver/GoogleMapssilver/reviews-estados-silver/\"\n",
        "\n",
        "# Obtener la lista de carpetas\n",
        "adls_files_reviews = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\n",
        "    .listStatus(spark._jvm.org.apache.hadoop.fs.Path(f\"abfss://{container_name}.blob.core.windows.net/{directory_path_reviews}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Directorio de metadata\r\n",
        "directory_path_metadata = \"/silver/GoogleMapssilver/metadata-sitios-silver/\"\r\n",
        "\r\n",
        "# Obtener la lista de carpetas\r\n",
        "adls_files_metadata = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\r\n",
        "    .listStatus(spark._jvm.org.apache.hadoop.fs.Path(f\"abfss://{container_name}.blob.core.windows.net/{directory_path_metadata}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7590116d-d7c4-4030-b2db-c5f037de4e71",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creamos una variable `new_files` que contiene los archivos que no estan en ADLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4952e931-aeb0-4500-b39b-894655a86af8",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "processed_files_reviews = spark.sql(\"SELECT file_name FROM processed_files_reviews_google_silver\").toPandas()[\"file_name\"].tolist()\n",
        "\n",
        "new_files_reviews = [file.getPath().getName() for file in adls_files_reviews if file.getPath().getName() not in processed_files_reviews]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "processed_files_metadata = spark.sql(\"SELECT file_name FROM processed_files_reviews_google_silver\").toPandas()[\"file_name\"].tolist()\r\n",
        "\r\n",
        "new_files_metadata = [file.getPath().getName() for file in adls_files_metadata if file.getPath().getName() not in processed_files_metadata]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "999d8908-3428-4a24-98ab-2de9dee9c966",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos ver que archivos no estan procesados aun."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "75125273-566b-45ce-b37e-0284a8b48794",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "for file in new_files_reviews:\r\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "for file in new_files_metadata:\r\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Cargamos el archivo metadata correspondiente de la tabla silver para poder procesar las tablas reviews-estados-silver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "path_metadata_silver = f\"abfss://datumtech@datumlake.dfs.core.windows.net/silver/GoogleMapssilver/metadata-sitios-silver/{new_files_metadata[0]}\"\r\n",
        "df_metadata = spark.read.format(\"parquet\").load(path_metadata_silver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df_metadata.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "file_metadata_gold = new_files_metadata[0].rstrip(\"-silver\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_metadata.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Cambiamos el tipo de dato de la columna 'num_of_reviews' para evitar problemas al momento de la ingesta de los datos en la base de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_metadata = df_metadata.withColumn(\"num_of_reviews\", col(\"num_of_reviews\").cast(\"int\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "path_metadata_gold = f\"abfss://datumtech@datumlake.dfs.core.windows.net/gold/GoogleMapsgold/metadata-sitios-gold/{file_metadata_gold}-gold\"\r\n",
        "df_metadata.write.format(\"parquet\").save(path_metadata_gold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "09bbff77-bdf5-494b-aa27-3f133d3650e4",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Creamos la funcion `gold` que realiza todo el proceso de agregacion de valor a la informacion y creacion de tablas secundarias para diferentes analisis y devuelve los dataframes en formato parquet a la tabla gold corrrespondiente a los datos ya procesados y enriquecidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "60781467-0b5a-4eac-a7e3-d77f53395a09",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "\n",
        "def gold(file_reviews):\n",
        "    \n",
        "    # Definimos las rutas:\n",
        "    file_reviews_gold = file_reviews.rstrip(\"silver\")\n",
        "    path_reviews_silver = f\"abfss://datumtech@datumlake.dfs.core.windows.net/silver/GoogleMapssilver/reviews-estados-silver/{file_reviews}\"\n",
        "    path_reviews_gold = f\"abfss://datumtech@datumlake.dfs.core.windows.net/gold/GoogleMapsgold/reviews-estados-gold/{file_reviews_gold}gold\"\n",
        "\n",
        "    # Cargamos los archivo desde la tabla silver en el storage.\n",
        "    df_reviews = spark.read.format(\"parquet\").load(path_reviews_silver)\n",
        "    \n",
        "    # La columna \"gmap_id\" en df_metadata ya contiene valores únicos, filtraremos df_reviews basándonos en la columna \"gmap_id\":\n",
        "    df_joined = df_reviews.join(df_metadata, on=\"gmap_id\", how=\"inner\")\n",
        "    df_reviews = df_joined.select(\"user_id\", \"gmap_id\", \"rating\", \"text\", \"date\")\n",
        "\n",
        "    # Cambiamos el tipo de dato de la columna 'rating' para evitar problemas al momento de la ingesta de los datos en la base de datos.\n",
        "    df_reviews = df_reviews.withColumn(\"rating\", col(\"rating\").cast(\"int\"))\n",
        "\n",
        "    # Truncamos la columna text a 7500 para evitar problema a la hora de cargar los datos en la base de datos.\n",
        "    df_reviews = df_reviews.withColumn(\"text\", substring(df_reviews[\"text\"], 1, 7000))\n",
        "\n",
        "    # Cambiamos el tipo de dato de la columna 'rating' para evitar problemas al momento de la ingesta de los datos en la base de datos.\n",
        "    df_reviews = df_reviews.withColumn(\"rating\", col(\"rating\").cast(\"int\"))\n",
        "\n",
        "    # Generamos la columna review_id con valores unicos para ser usada com primary key\n",
        "    df_reviews = df_reviews.withColumn('review_id', concat(df_reviews['user_id'], df_reviews['gmap_id']))\n",
        "\n",
        "    # Guardamos los DataFrames df_reviews y df_metadata en la tabla gold correspondiente a los datos procesados y enriquecidos.   \n",
        "    return df_reviews.write.format(\"parquet\").save(path_reviews_gold)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Aplicamos la funcion `gold` a todos los archivos sin procesar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e97045d1-6c7a-4bcb-86ab-c765f478c4d2",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "for file in new_files_reviews:\n",
        "    gold(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e069d029-6178-43a1-91d8-c496905db21b",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "#### Para comprobar que todo se haya ejecutado de manera correcta, podemos traer cualquier archivo de la tabla gold y hacer algunas verificaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "path_reviews_gold = f\"abfss://datumtech@datumlake.dfs.core.windows.net/gold/GoogleMapsgold/reviews-estados-gold/review-Florida-gold\"\r\n",
        "df_reviews_gold = spark.read.format(\"parquet\").load(path_reviews_gold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_reviews_gold.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_reviews_gold.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1b5de4c7-c23f-4032-b8e9-5442e40e4b0f",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Agregamos a la tabla `processed_files_reviews_google_silver` y `processed_files_metadata_google_silver` los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2dfc0dd7-7d0f-4818-9f0d-de05fc7101f6",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "new_files_df = spark.createDataFrame([(file,) for file in new_files_reviews], [\"file_name\"])\n",
        "new_files_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"processed_files_reviews_google_silver\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "new_files_df = spark.createDataFrame([(file,) for file in new_files_metadata], [\"file_name\"])\r\n",
        "new_files_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"processed_files_metadata_google_silver\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ff629741-cace-4a0d-94ed-476f9c6f0054",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos verificar que, efectivamente esten registrados los archivos ya procesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "793d5dee-6b48-4949-a242-44b891b18d9b",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM processed_files_reviews_google_silver \").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM processed_files_metadata_google_silver \").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "76a101ca-0bbf-4b98-93bc-aa9068291bc3",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "### Podemos verificar si hay nulos en algun archivo en la tabla silver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "704b7ffd-91fe-42a6-b1f6-661d412d746d",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Funcion para el conteo de nulos del dataframe.\n",
        "'''def null_counts (df):\n",
        "    counts = df.select([sum(col(c).isNull().cast(\"integer\")).alias(c) for c in df.columns])\n",
        "    return counts.show()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f135b019-6319-42b7-af36-cb888a5521b6",
          "showTitle": false,
          "title": ""
        }
      },
      "source": [
        "# Vemos que columnas poseen nulos y en que cantidad.\n",
        "#nulls = null_counts(df)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}